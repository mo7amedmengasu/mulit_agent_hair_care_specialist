{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192a0626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervisor LLM raw decision: 'image_analyzer'\n",
      "Image agent output: As an AI, I am unable to view or process images from local file paths like \"test.png.\" Therefore, I cannot provide a hair type label or observation based on an image I cannot see.\n",
      "\n",
      "If you can describe the hair in the image, I would be happy to provide an assessment!\n",
      "Supervisor LLM raw decision: 'done'\n",
      "GRAPH RESPONSE KEYS: dict_keys(['messages', 'next_agent', 'Images_data', 'final_answer', 'task_complete', 'current_task'])\n",
      "FINAL ANSWER (if present):\n",
      " It appears I wasn't able to process the image you provided, as I cannot directly view files from your local computer. As an AI, my capabilities are limited to text-based input, so I can't 'see' images like a human can. To help me understand the hair type, please describe it in detail. You could mention its texture (straight, wavy, curly, coily), thickness (fine, medium, thick), and any specific characteristics like frizz, oiliness, or dryness. Once you provide a description, I'll be happy to offer an assessment and relevant insights. This will allow me to give you the best possible hair type label and observations.\n",
      "\n",
      "**Actionable Tip:** Describe the hair's texture, thickness, and any key characteristics for an accurate assessment.\n"
     ]
    }
   ],
   "source": [
    "# corrected_multi_agent.py\n",
    "from typing import List, Dict, Literal\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage\n",
    "from langgraph.graph import StateGraph, END, MessagesState\n",
    "from langgraph.graph import add_messages\n",
    "from typing import Annotated\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from datetime import datetime\n",
    "\n",
    "# ---- LLM (your existing LLM)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.3)\n",
    "\n",
    "# ---- State\n",
    "class SupervisorState(MessagesState):\n",
    "    # attach messages using add_messages helper so the graph can pass messages in\n",
    "    messages: Annotated[List[BaseMessage], add_messages] = []\n",
    "    next_agent: str = \"supervisor\"\n",
    "    Images_data: str = \"\"\n",
    "    scientific_data: str = \"\"\n",
    "    products_data: str = \"\"\n",
    "    final_answer: str = \"\"\n",
    "    task_complete: bool = False\n",
    "    current_task: str = \"\"\n",
    "\n",
    "# ---- Helper: normalized mapping from supervisor token -> node name\n",
    "SUPERVISOR_TO_NODE = {\n",
    "    \"image_analyzer\": \"image_analysis_agent\",\n",
    "    \"scientific_researcher\": \"scientific_data_agent\",\n",
    "    \"products_expert\": \"products_data_agent\",\n",
    "    \"done\": \"final_answer_agent\",\n",
    "}\n",
    "\n",
    "# ---- Supervisor (robust)\n",
    "def supervisor_agent(state: SupervisorState) -> Dict:\n",
    "    messages = state.get(\"messages\", [])\n",
    "    task = messages[-1].content if messages else \"\"\n",
    "\n",
    "    # flags\n",
    "    has_image = bool(state.get(\"Images_data\"))\n",
    "    has_scientific = bool(state.get(\"scientific_data\"))\n",
    "    has_product_info = bool(state.get(\"products_data\"))\n",
    "    has_final_answer = bool(state.get(\"final_answer\"))\n",
    "\n",
    "    # Build a short, strict prompt (ask LLM to return exactly one token)\n",
    "    system = SystemMessage(\n",
    "        content=(\n",
    "            \"You are a supervisor. Decide which single agent to run next. \"\n",
    "            \"Respond with exactly one of: image_analyzer / scientific_researcher / products_expert / DONE.\\n\"\n",
    "            \"Interpret these booleans: has_image, has_scientific, has_product_info, has_final_answer.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    human = HumanMessage(\n",
    "        content=(\n",
    "            f\"Task: {task}\\n\"\n",
    "            f\"has_image: {has_image}\\n\"\n",
    "            f\"has_scientific: {has_scientific}\\n\"\n",
    "            f\"has_product_info: {has_product_info}\\n\"\n",
    "            f\"has_final_answer: {has_final_answer}\\n\\n\"\n",
    "            \"Which agent next? (one word from image_analyzer / scientific_researcher / products_expert / DONE)\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    decision = llm.invoke([system, human])\n",
    "    decision_text = decision.content.strip().lower()\n",
    "    print(\"Supervisor LLM raw decision:\", repr(decision_text))\n",
    "\n",
    "    # sanitize\n",
    "    choice = None\n",
    "    for token in SUPERVISOR_TO_NODE.keys():\n",
    "        if token in decision_text:\n",
    "            choice = token\n",
    "            break\n",
    "    if not choice:\n",
    "        # fallback heuristic\n",
    "        if not has_image:\n",
    "            choice = \"image_analyzer\"\n",
    "        elif not has_scientific:\n",
    "            choice = \"scientific_researcher\"\n",
    "        elif not has_product_info:\n",
    "            choice = \"products_expert\"\n",
    "        else:\n",
    "            choice = \"done\"\n",
    "\n",
    "    next_node = SUPERVISOR_TO_NODE[choice]\n",
    "\n",
    "    # human-friendly message to store in messages\n",
    "    mapping_msg = {\n",
    "        \"image_analyzer\": \"📋 Supervisor: Assign to image analysis.\",\n",
    "        \"scientific_researcher\": \"📋 Supervisor: Assign to scientific research.\",\n",
    "        \"products_expert\": \"📋 Supervisor: Assign to product expert.\",\n",
    "        \"done\": \"✅ Supervisor: All tasks complete, preparing final answer.\"\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=mapping_msg[choice])],\n",
    "        \"next_agent\": next_node,\n",
    "        \"current_task\": task\n",
    "    }\n",
    "\n",
    "# ---- Image analysis agent (placeholder implementation)\n",
    "def image_analysis_agent(state: SupervisorState) -> Dict:\n",
    "    # Use the provided image path in state.current_task or Images_data; here we simulate an LLM analysis.\n",
    "    # In your production code you should replace this with proper image uploading + vision model call.\n",
    "    image_path = state.get(\"Images_data\", \"\") or \"test.png\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are an expert in hair care. We have an image at path: {image_path}.\\n\"\n",
    "        \"Based on that image, give a single-line hair type label (choose short) and a 1-2 sentence observation.\"\n",
    "    )\n",
    "    resp = llm.invoke([HumanMessage(content=prompt)])\n",
    "    analysis_text = resp.content.strip()\n",
    "\n",
    "    state_update = {\n",
    "        \"messages\": [AIMessage(content=f\"🖼️ Image Analysis Agent: {analysis_text}\")],\n",
    "        \"Images_data\": analysis_text,\n",
    "        \"next_agent\": \"supervisor\"\n",
    "    }\n",
    "    print(\"Image agent output:\", analysis_text)\n",
    "    return state_update\n",
    "\n",
    "# ---- Scientific data agent (placeholder - use your retriever here)\n",
    "def scientific_data_agent(state: SupervisorState) -> Dict:\n",
    "    task = state.get(\"current_task\", \"general haircare\")\n",
    "    prompt = (\n",
    "        f\"As a scientific researcher summarize top 3 evidence-backed points about: {task}.\\n\"\n",
    "        \"Return a short paragraph and list 1-2 citations (if none available say 'no citations').\"\n",
    "    )\n",
    "    resp = llm.invoke([HumanMessage(content=prompt)])\n",
    "    doc_text = resp.content.strip()\n",
    "\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"🔬 Scientific Agent: Retrieved data.\")],\n",
    "        \"scientific_data\": doc_text,\n",
    "        \"next_agent\": \"supervisor\"\n",
    "    }\n",
    "\n",
    "# ---- Products data agent (your product catalog)\n",
    "def products_data_agent(state: SupervisorState) -> Dict:\n",
    "    # You can replace with real DB/API lookup. This is a safe placeholder returning a small product list.\n",
    "    products_data = (\n",
    "        \"Gliss Ultimate Repair - Liquid Keratin: for very damaged hair.\\n\"\n",
    "        \"Gliss Aqua Revive - Hyaluron Complex: for dry hair hydration.\\n\"\n",
    "        \"Gliss Oil Nutritive - Marula Oil: for frizzy long hair.\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"🛍️ Products Agent: Provided product recommendations.\")],\n",
    "        \"products_data\": products_data,\n",
    "        \"next_agent\": \"supervisor\"\n",
    "    }\n",
    "\n",
    "# ---- Final answer agent (aggregates and produces final answer)\n",
    "def final_answer_agent(state: SupervisorState) -> Dict:\n",
    "    images_data = state.get(\"Images_data\", \"No image data available.\")\n",
    "    scientific_data = state.get(\"scientific_data\", \"No scientific data available.\")\n",
    "    products_data = state.get(\"products_data\", \"No product data available.\")\n",
    "    task = state.get(\"current_task\", \"the task\")\n",
    "\n",
    "    prompt = (\n",
    "        f\"Create a short user-friendly answer for: {task}\\n\\n\"\n",
    "        f\"Image analysis:\\n{images_data}\\n\\n\"\n",
    "        f\"Scientific summary:\\n{scientific_data}\\n\\n\"\n",
    "        f\"Product suggestions:\\n{products_data}\\n\\n\"\n",
    "        \"Return a 5-8 sentence final recommendation and a 1-line actionable tip.\"\n",
    "    )\n",
    "    resp = llm.invoke([HumanMessage(content=prompt)])\n",
    "    final = resp.content.strip()\n",
    "\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"✅ Final Answer Agent: {final}\")],\n",
    "        \"final_answer\": final,\n",
    "        \"task_complete\": True,\n",
    "        \"next_agent\": \"end\"\n",
    "    }\n",
    "\n",
    "# ---- Router (maps the next_agent string to nodes)\n",
    "def router(state: SupervisorState) -> Literal[\"supervisor\", \"image_analysis_agent\", \"scientific_data_agent\", \"products_data_agent\", \"final_answer_agent\", \"__end__\"]:\n",
    "    next_agent = state.get(\"next_agent\", \"supervisor\")\n",
    "    if next_agent == \"end\" or state.get(\"task_complete\", False):\n",
    "        return END\n",
    "\n",
    "    if next_agent in [\"supervisor\", \"image_analysis_agent\", \"scientific_data_agent\", \"products_data_agent\", \"final_answer_agent\"]:\n",
    "        return next_agent\n",
    "    return \"supervisor\"\n",
    "\n",
    "# ---- Build workflow\n",
    "workflow = StateGraph(SupervisorState)\n",
    "workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "workflow.add_node(\"image_analysis_agent\", image_analysis_agent)\n",
    "workflow.add_node(\"scientific_data_agent\", scientific_data_agent)\n",
    "workflow.add_node(\"products_data_agent\", products_data_agent)\n",
    "workflow.add_node(\"final_answer_agent\", final_answer_agent)\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "for node in [\"supervisor\", \"image_analysis_agent\", \"scientific_data_agent\", \"products_data_agent\", \"final_answer_agent\"]:\n",
    "    workflow.add_conditional_edges(\n",
    "        node,\n",
    "        router,\n",
    "        {\n",
    "            \"supervisor\": \"supervisor\",\n",
    "            \"image_analysis_agent\": \"image_analysis_agent\",\n",
    "            \"scientific_data_agent\": \"scientific_data_agent\",\n",
    "            \"products_data_agent\": \"products_data_agent\",\n",
    "            \"final_answer_agent\": \"final_answer_agent\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# ---- Example run: pass an initial HumanMessage with an actual task\n",
    "if __name__ == \"__main__\":\n",
    "    # Provide an initial task message so supervisor has something to act on\n",
    "    initial = HumanMessage(content=\"Assess hair condition from an uploaded image and recommend a product for dry, damaged long hair.\")\n",
    "    response = graph.invoke(initial)\n",
    "    print(\"GRAPH RESPONSE KEYS:\", response.keys())\n",
    "    print(\"FINAL ANSWER (if present):\\n\", response.get(\"final_answer\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
